{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques\n",
        "**Question 1**\n",
        "\n",
        "What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "**Answer:** Boosting is an ensemble technique that builds a strong learner by sequentially combining many weak learners (models that perform just slightly better than random). Each new model focuses on examples the previous models struggled with, typically by reweighting samples (e.g., AdaBoost) or by fitting residual errors/gradients (e.g., Gradient Boosting). Final predictions are a weighted vote (classification) or sum (regression) of all weak learners.\n",
        "\n",
        "Why it improves weak learners:\n",
        "\n",
        "- Focus on hard cases: Misclassified or high-error samples get more attention in later rounds.\n",
        "\n",
        "- Bias reduction: Sequential corrections reduce systematic errors.\n",
        "\n",
        "- Additive modeling: Each learner adds a small, targeted improvement.\n",
        "\n",
        "- Shrinkage & regularization: Learning rate, depth constraints, and penalties curb overfitting while allowing many learners to contribute.\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "**Answer:**\n",
        "| Aspect                 | **AdaBoost**                                                                         | **Gradient Boosting**                                                                     |\n",
        "| ---------------------- | ------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------- |\n",
        "| **Core idea**          | Reweights misclassified samples and builds next learner on updated weights.          | Fits new learners to the **residuals (negative gradients)** of the chosen loss function.  |\n",
        "| **Error handling**     | Increases weights of wrongly classified samples so the next learner focuses on them. | Each learner directly minimizes the gradient of the loss (e.g., squared error, log loss). |\n",
        "| **Loss function**      | Implicitly based on **exponential loss** (for classification).                       | Works with any **differentiable loss** (squared error, log loss, MAE, etc.).              |\n",
        "| **Base learners**      | Often **decision stumps** (depth=1 trees).                                           | Typically **deeper trees** (depth 3–6) for capturing complex interactions.                |\n",
        "| **Combination method** | Weighted majority vote (classification) or weighted sum (regression).                | Additive model: final prediction is the sum of all learners’ outputs.                     |\n",
        "| **Flexibility**        | Less flexible (fixed exponential-style weighting).                                   | More flexible (supports custom loss, regularization, shrinkage).                          |\n",
        "| **Interpretability**   | Simpler, fewer hyperparameters.                                                      | Richer, more complex, requires careful tuning.                                            |\n",
        "| **Performance**        | Good for simpler tasks, small datasets.                                              | Superior for large, complex, noisy, or tabular datasets.                                  |\n",
        "\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "How does regularization help in XGBoost?\n",
        "\n",
        "**Answer:**\n",
        "XGBoost adds multiple regularization mechanisms that reduce overfitting and improve generalization/performance:\n",
        "\n",
        "- L1/L2 penalties (alpha, lambda) on leaf weights stabilize the model and control complexity.\n",
        "\n",
        "- Tree constraints: max_depth, min_child_weight, gamma (min loss reduction for a split), and max_leaves limit tree growth.\n",
        "\n",
        "- Shrinkage (learning_rate/eta): Small learning steps make each tree’s contribution modest, enabling more robust additive modeling.\n",
        "\n",
        "- Subsampling: subsample (rows) and colsample_bytree/bylevel/bynode (features) act like built-in bagging to reduce variance.\n",
        "\n",
        "- Early stopping: Halts training when validation performance stalls, preventing overfit.\n",
        "\n",
        "Together, these mechanisms make XGBoost fast, accurate, and robust on noisy/tabular data.\n",
        "\n",
        "**Question 4**\n",
        "\n",
        "Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "**Answer:**\n",
        "CatBoost is designed for categorical features and avoids target leakage via ordered target statistics: it computes target-based encodings using permutation-driven, past-only statistics. Key advantages:\n",
        "\n",
        "- Native categorical handling: No need for heavy one-hot encoding; reduces dimensionality.\n",
        "\n",
        "- Ordered boosting: Mitigates prediction shift by using an ordering scheme for both boosting and statistics.\n",
        "\n",
        "- Robust defaults: Handles missing values and categorical encodings internally with strong regularization.\n",
        "\n",
        "- Fast, accurate on tabular data: Typically competitive out-of-the-box with minimal preprocessing.\n",
        "\n",
        "**Question 5**\n",
        "\n",
        "What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "**Answer:**\n",
        "Boosting often wins on structured/tabular problems with heterogeneous features and complex interactions, especially when accuracy is paramount:\n",
        "\n",
        "- Credit risk / fraud detection (imbalanced classification)\n",
        "\n",
        "- Click-through rate (CTR) prediction / ad ranking\n",
        "\n",
        "- Customer churn / propensity modeling\n",
        "\n",
        "- Insurance pricing & claim prediction\n",
        "\n",
        "- Medical diagnosis / readmission risk\n",
        "\n",
        "- Demand forecasting / price elasticity\n",
        "\n",
        "Reason: Boosting’s additive, gradient-driven corrections capture subtle patterns better than bagging’s pure variance reduction."
      ],
      "metadata": {
        "id": "MbrYC1QqdiBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**\n",
        "\n",
        "Write a Python program to:\n",
        "\n",
        "Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "Print the model accuracy"
      ],
      "metadata": {
        "id": "n2gEqlqPIvuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Base learner: decision stump (depth=1) is classic for AdaBoost\n",
        "base = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "\n",
        "# AdaBoost\n",
        "clf = AdaBoostClassifier(\n",
        "    estimator=base,        # or base_estimator in older sklearn\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(f\"AdaBoost test accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb50CUqeKY7f",
        "outputId": "82c28ab5-0f5b-476c-a7e2-7920dd04450f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost test accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7**\n",
        "\n",
        "Write a Python program to:\n",
        "\n",
        "Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "K2jcUksrKLAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.frame.drop(columns=[\"MedHouseVal\"])\n",
        "y = data.frame[\"MedHouseVal\"]\n",
        "\n",
        "# (Optional) simple scaling for continuous features can help weak learners\n",
        "num_features = list(X.columns)\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[(\"num\", StandardScaler(), num_features)],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "pipe = Pipeline([(\"pre\", pre), (\"model\", gbr)])\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "print(f\"Gradient Boosting R^2: {r2_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2_N7xliKYfi",
        "outputId": "2ac31dc3-40bf-4e7f-d042-0785b6e07b73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting R^2: 0.8077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8**\n",
        "\n",
        "Write a Python program to:\n",
        "\n",
        "Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "Tune the learning rate using GridSearchCV\n",
        "\n",
        "Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "Xn992kZPKLVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install xgboost  # Uncomment if running locally and xgboost is missing\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Base model\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Grid over learning_rate\n",
        "param_grid = {\"learning_rate\": [0.01, 0.05, 0.1, 0.2]}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "\n",
        "print(f\"Best params: {grid.best_params_}\")\n",
        "print(f\"Test accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dlDozlaKZeK",
        "outputId": "40525572-6c10-44ff-d1ba-40009c175462"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'learning_rate': 0.1}\n",
            "Test accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**\n",
        "\n",
        "Write a Python program to:\n",
        "\n",
        "Train a CatBoost Classifier\n",
        "\n",
        "Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "2GaCd4h6KLoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKZg_ogdLEyn",
        "outputId": "dcaf9b1f-0604-4ae2-b2af-08852b7cc534"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9: CatBoost Classifier + Confusion Matrix\n",
        "# !pip install catboost seaborn  # Uncomment if running locally\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# CatBoost (silent training)\n",
        "cb = CatBoostClassifier(\n",
        "    depth=6,\n",
        "    learning_rate=0.1,\n",
        "    iterations=400,\n",
        "    loss_function=\"Logloss\",\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "cb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and confusion matrix\n",
        "y_pred = cb.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\n",
        "plt.title(\"CatBoost Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "XecQP3lZKX77",
        "outputId": "8ec6fb46-c29c-4970-fb03-046cfd6e1af3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        42\n",
            "           1       0.96      0.99      0.97        72\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10**\n",
        "\n",
        "You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "Data preprocessing & handling missing/categorical values\n",
        "\n",
        "Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "Hyperparameter tuning strategy\n",
        "\n",
        "Evaluation metrics you'd choose and why\n",
        "\n",
        "How the business would benefit from your model"
      ],
      "metadata": {
        "id": "4BLSMkpcKL-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10: End-to-end pipeline sketch for imbalanced loan default\n",
        "# !pip install catboost imbalanced-learn  # if needed\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTENC  # use only if necessary and *train-only*\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Example: Suppose df has mixed types\n",
        "# df = pd.read_csv(\"loan_data.csv\")\n",
        "# target = \"default\"\n",
        "# cat_cols = [\"gender\", \"city\", \"employment_type\"]\n",
        "# num_cols = [\"age\", \"income\", \"balance\", \"tx_count_30d\", \"max_utilization\"]\n",
        "\n",
        "# For illustration, we'll create placeholders:\n",
        "df = pd.DataFrame({\n",
        "    \"gender\": np.random.choice([\"M\", \"F\"], 2000),\n",
        "    \"city\": np.random.choice([\"A\", \"B\", \"C\"], 2000),\n",
        "    \"employment_type\": np.random.choice([\"salaried\", \"self\"], 2000),\n",
        "    \"age\": np.random.normal(35, 8, 2000),\n",
        "    \"income\": np.abs(np.random.normal(60000, 20000, 2000)),\n",
        "    \"balance\": np.abs(np.random.normal(15000, 12000, 2000)),\n",
        "    \"tx_count_30d\": np.random.poisson(12, 2000),\n",
        "    \"max_utilization\": np.random.beta(2, 5, 2000),\n",
        "})\n",
        "# Imbalanced target (10% defaults)\n",
        "rng = np.random.RandomState(42)\n",
        "df[\"default\"] = (rng.rand(2000) < 0.10).astype(int)\n",
        "\n",
        "target = \"default\"\n",
        "cat_cols = [\"gender\", \"city\", \"employment_type\"]\n",
        "num_cols = [\"age\", \"income\", \"balance\", \"tx_count_30d\", \"max_utilization\"]\n",
        "\n",
        "X = df[cat_cols + num_cols]\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# CatBoost: handles missing + categorical internally\n",
        "# We'll still impute numerics to be explicit; cat features can stay as-is (strings)\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "# Build numerical preprocessing (optional; CatBoost can take NaN directly)\n",
        "# We'll convert to numpy arrays for CatBoost Pool.\n",
        "X_train_num = num_imputer.fit_transform(X_train[num_cols])\n",
        "X_test_num = num_imputer.transform(X_test[num_cols])\n",
        "\n",
        "# Reassemble matrices with categorical columns (as object arrays)\n",
        "X_train_proc = pd.concat([X_train[cat_cols].reset_index(drop=True),\n",
        "                          pd.DataFrame(X_train_num, columns=num_cols)], axis=1)\n",
        "X_test_proc = pd.concat([X_test[cat_cols].reset_index(drop=True),\n",
        "                         pd.DataFrame(X_test_num, columns=num_cols)], axis=1)\n",
        "\n",
        "# Identify categorical feature indices for CatBoost\n",
        "cat_indices = [X_train_proc.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=2000,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3.0,\n",
        "    subsample=0.8,\n",
        "    loss_function=\"Logloss\",\n",
        "    eval_metric=\"AUC\",\n",
        "    random_seed=42,\n",
        "    class_weights=[1.0, 5.0],  # weight positive class more (tune based on prevalence/costs)\n",
        "    od_type=\"Iter\",\n",
        "    od_wait=100,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cb.fit(\n",
        "    X_train_proc, y_train,\n",
        "    cat_features=cat_indices,\n",
        "    eval_set=(X_test_proc, y_test),\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "# Probabilities & metrics\n",
        "p_test = cb.predict_proba(X_test_proc)[:, 1]\n",
        "roc = roc_auc_score(y_test, p_test)\n",
        "pr = average_precision_score(y_test, p_test)\n",
        "brier = brier_score_loss(y_test, p_test)\n",
        "\n",
        "print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr:.4f} | Brier: {brier:.4f}\")\n",
        "\n",
        "# (Optional) calibration if you need well-calibrated probabilities:\n",
        "# Calibrate via Platt scaling/Isotonic on validation folds (sketch):\n",
        "# calib = CalibratedClassifierCV(cb, method=\"isotonic\", cv=3)\n",
        "# calib.fit(X_train_proc, y_train)\n",
        "# p_cal = calib.predict_proba(X_test_proc)[:, 1]\n",
        "\n",
        "# (Optional) choose decision threshold based on business cost curve or F-beta:\n",
        "# threshold = 0.25\n",
        "# y_pred = (p_test >= threshold).astype(int)\n",
        "# from sklearn.metrics import classification_report\n",
        "# print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucsOHrxjKXjP",
        "outputId": "e8a0a0b6-2cfd-43aa-f362-e38e5b9e8901"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.5500 | PR-AUC: 0.1187 | Brier: 0.2251\n"
          ]
        }
      ]
    }
  ]
}