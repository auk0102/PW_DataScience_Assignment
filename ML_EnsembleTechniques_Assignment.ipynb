{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Techniques\n",
        "Question 1:\n",
        "What is an ensemble method in machine learning? Explain why ensemble methods often perform better than individual models.\n",
        "\n",
        "Question 2: What is differentiate between Bagging and Boosting techniques.\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree .\n",
        "\n",
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "\n",
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "rKUJ6Mb1b-cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Solutions"
      ],
      "metadata": {
        "id": "npyO9vtLgGUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:**\n",
        "What is an ensemble method in machine learning? Explain why ensemble methods often perform better than individual models.\n",
        "\n",
        "**Answer:**\n",
        "An ensemble method in machine learning combines predictions from multiple models to produce a single, stronger prediction. The main idea is that by aggregating multiple weak learners (models that perform slightly better than random), the ensemble reduces errors and improves overall performance.\n",
        "\n",
        "Why ensemble methods perform better:\n",
        "\n",
        "Reduction of variance: Combining multiple models reduces overfitting compared to a single complex model (e.g., Random Forest reduces variance of decision trees).\n",
        "\n",
        "Reduction of bias: Some ensemble methods (e.g., Boosting) can combine weak models to reduce bias.\n",
        "\n",
        "Improved generalization: Aggregating predictions helps the model perform better on unseen data.\n",
        "\n",
        "Error cancellation: Individual models may make errors on different samples, which can cancel out in the ensemble.\n",
        "\n",
        "**Question 2:** What is differentiate between Bagging and Boosting techniques.\n",
        "\n",
        "| Feature         | Bagging                          | Boosting                                 |\n",
        "| --------------- | -------------------------------- | ---------------------------------------- |\n",
        "| Full form       | Bootstrap Aggregating            | Sequential Boosting                      |\n",
        "| Base model      | Independent, often same type     | Sequential, each depends on prior errors |\n",
        "| Goal            | Reduce variance                  | Reduce bias and variance                 |\n",
        "| Sampling        | Random sampling with replacement | Weighted sampling based on errors        |\n",
        "| Examples        | Random Forest                    | AdaBoost, Gradient Boosting              |\n",
        "| Parallelization | Can be parallelized              | Sequential, cannot fully parallelize     |\n",
        "\n",
        "\n",
        "**Question 3:** What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "**Answer:**\n",
        "Bootstrap sampling is a technique of randomly selecting samples from the dataset with replacement, so that some samples may appear multiple times while others may not appear at all.\n",
        "\n",
        "Role in Bagging:\n",
        "\n",
        "Creates multiple diverse datasets for training base models.\n",
        "\n",
        "Ensures each tree in Random Forest is trained on a slightly different dataset, reducing variance.\n",
        "\n",
        "Allows calculation of Out-of-Bag (OOB) error using samples not included in the bootstrap sample.\n",
        "\n",
        "**Question 4:** What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "**Answer:**\n",
        "OOB samples are the data points that were not included in a bootstrap sample used to train a particular base model.\n",
        "\n",
        "OOB score: Random Forest can evaluate its performance using OOB samples without needing a separate test set. For each sample, predictions from all trees that did not include this sample in training are aggregated, and the accuracy (or other metric) is calculated.\n",
        "\n",
        "Advantage: Efficient validation and helps detect overfitting.\n",
        "\n",
        "**Question 5:** Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "| Aspect       | Single Decision Tree                        | Random Forest                             |\n",
        "| ------------ | ------------------------------------------- | ----------------------------------------- |\n",
        "| Stability    | Unstable: Small data changes may alter tree | More stable: Averaging over many trees    |\n",
        "| Accuracy     | Less reliable                               | More reliable                             |\n",
        "| Feature bias | Can be biased to features with many levels  | Reduces bias due to multiple trees        |\n",
        "| Insight      | Simple, interpretable                       | Aggregate importance gives robust insight |\n"
      ],
      "metadata": {
        "id": "SndORHyQe8LE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "jfsQvxpdfqgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importance = pd.Series(rf.feature_importances_, index=feature_names)\n",
        "top5_features = importance.sort_values(ascending=False).head(5)\n",
        "print(\"Top 5 important features:\\n\", top5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VIzFgYmIgDhx",
        "outputId": "838c98a7-b9e1-4e9d-be5d-a5a06faeab6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 important features:\n",
            " worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree .\n",
        "\n"
      ],
      "metadata": {
        "id": "G2SwRtz9f3uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_preds = dt_model.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_preds)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees as base learners\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),  # use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bag_model.fit(X_train, y_train)\n",
        "bag_preds = bag_model.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_preds)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Accuracy: {:.4f}\".format(dt_acc))\n",
        "print(\"Bagging Classifier Accuracy: {:.4f}\".format(bag_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ht5aj3B3gSdT",
        "outputId": "12315db3-1948-41c4-b46f-ea0f2b27b48d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9333\n",
            "Bagging Classifier Accuracy: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "A55xaltPf4Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [2, 4, 6, None]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_rf = grid.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z-sR2BvjgwC5",
        "outputId": "fc5c7e33-81fa-471b-ab79-8a65d4d4df32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 150}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "\n"
      ],
      "metadata": {
        "id": "_wofBkv2f96k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),  # use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bag_reg.fit(X_train, y_train)\n",
        "mse_bag = mean_squared_error(y_test, bag_reg.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "mse_rf = mean_squared_error(y_test, rf_reg.predict(X_test))\n",
        "\n",
        "print(f\"MSE Bagging Regressor: {mse_bag:.4f}\")\n",
        "print(f\"MSE Random Forest Regressor: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9-IOyFh9li6m",
        "outputId": "a90e5d86-190f-4586-b9e8-0f5b43daf224"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Bagging Regressor: 0.2579\n",
            "MSE Random Forest Regressor: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "onR2RSG8gBph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-step approach:**\n",
        "\n",
        "Choose between Bagging or Boosting\n",
        "\n",
        "If dataset has high variance → Bagging (e.g., Random Forest)\n",
        "\n",
        "If dataset has high bias or weak predictors → Boosting (e.g., XGBoost, AdaBoost)\n",
        "\n",
        "Handle overfitting\n",
        "\n",
        "Limit tree depth, number of estimators, or use regularization in boosting.\n",
        "\n",
        "Use cross-validation to check generalization.\n",
        "\n",
        "Select base models\n",
        "\n",
        "Decision Trees are common base models due to interpretability and flexibility.\n",
        "\n",
        "Gradient Boosting or Random Forests can handle both categorical and numerical data.\n",
        "\n",
        "Evaluate performance using cross-validation\n",
        "\n",
        "Use k-fold CV for robust evaluation.\n",
        "\n",
        "Metrics: Accuracy, ROC-AUC, F1-score (for imbalanced data).\n",
        "\n",
        "Justification for ensemble learning\n",
        "\n",
        "Reduces variance (bagging) or bias (boosting).\n",
        "\n",
        "Aggregated predictions improve accuracy and stability.\n",
        "\n",
        "Helps in making reliable loan approval decisions and minimizing financial risk."
      ],
      "metadata": {
        "id": "glrJyaZPqNlB"
      }
    }
  ]
}